{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "from bisect import bisect_left\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from lobio.lob.limit_order import Order, AMOUNT_TICK, PRICE_TICK, EventType, OrderType, TraderId, Side\n",
    "from lobio.lob.order_book import  OrderBook, OrderBookSimple, OrderBookSimple2\n",
    "from lobio.utils.utils import group_diffs, group_historical_trades, group_orders, get_initial_order_book\n",
    "from queue_dynamic.models.models import GaussianPDFModel, ModelPerceptron, Critic\n",
    "from queue_dynamic.simulation import run_true_simulation, run_pred_simulation\n",
    "from queue_dynamic.losses import reinforce_objective, reinforce_objective_with_baseline, \\\n",
    "    critic_objective_td, critic_objective_mse, actor_objective, ppo_objective\n",
    "\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "\n",
    "%load_ext line_profiler\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs_prepared_file = \"../data/diffs_prepared.npy\"\n",
    "init_lob_prepared_file = \"../data/init_lob_prepared.npy\"\n",
    "orders_prepared_file = \"../data/orders_prepared.npy\"\n",
    "pl_to_enter_file = \"../data/price_level_to_enter.npy\"\n",
    "\n",
    "with open(init_lob_prepared_file, 'rb') as file:\n",
    "    init_lob = np.load(file)\n",
    "with open(diffs_prepared_file, 'rb') as file:\n",
    "    diffs = np.load(file)\n",
    "with open(orders_prepared_file, 'rb') as file:\n",
    "    orders = np.load(file)\n",
    "\n",
    "with open(pl_to_enter_file, 'rb') as file:\n",
    "    pl_to_enter = np.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23209/23209 [00:01<00:00, 11604.98it/s]\n",
      "100%|██████████| 23209/23209 [00:00<00:00, 80555.70it/s]\n"
     ]
    }
   ],
   "source": [
    "diffs_grouped = group_diffs(diffs)\n",
    "orders_per_diff = group_orders(orders, len(diffs_grouped))\n",
    "n_poses = len(pl_to_enter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_observation = 3\n",
    "dim_action = 1\n",
    "n_hidden_layers = 1\n",
    "dim_hidden = 8\n",
    "n_hidden_layers_critic = 1\n",
    "dim_hidden_critic = 8\n",
    "std = 0.01\n",
    "scale_factor = 10\n",
    "gamma = 0.95\n",
    "lr_critic = 0.005\n",
    "lr = 0.001\n",
    "N_iter = 20\n",
    "N_episode = 500\n",
    "n_exp = 10\n",
    "N_td = 1\n",
    "N_critic_epoch = 30\n",
    "epsilon = 0.2\n",
    "N_ppo_epoch = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = int(round(np.random.uniform(0, 10), 5) * 10**5)\n",
    "torch.manual_seed(seed)\n",
    "rng = np.random.default_rng(seed)\n",
    "\n",
    "agent_network = ModelPerceptron(dim_observation, dim_action, dim_hidden=dim_hidden, n_hidden_layers=n_hidden_layers)\n",
    "rl_agent = GaussianPDFModel(\n",
    "    model=agent_network,\n",
    "    dim_observation=dim_observation,\n",
    "    dim_action=dim_action,\n",
    "    action_bounds=np.array([[0, 1]]),\n",
    "    scale_factor=scale_factor,\n",
    "    std=std,\n",
    ")\n",
    "\n",
    "rl_critic = Critic(ModelPerceptron(dim_observation, 1, dim_hidden=dim_hidden_critic, n_hidden_layers=n_hidden_layers_critic))\n",
    "\n",
    "optimizer_agent = torch.optim.SGD(rl_agent.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Critic TD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:09<00:00,  3.06it/s]\n",
      "100%|██████████| 30/30 [00:09<00:00,  3.03it/s]\n",
      "100%|██████████| 30/30 [00:09<00:00,  3.30it/s]\n",
      "100%|██████████| 30/30 [00:18<00:00,  1.65it/s]\n",
      "100%|██████████| 30/30 [00:14<00:00,  2.04it/s]\n",
      "100%|██████████| 30/30 [00:09<00:00,  3.25it/s]\n",
      "100%|██████████| 30/30 [00:14<00:00,  2.06it/s]\n",
      "100%|██████████| 30/30 [00:14<00:00,  2.12it/s]\n",
      "100%|██████████| 30/30 [00:10<00:00,  2.79it/s]\n",
      "100%|██████████| 30/30 [00:15<00:00,  1.93it/s]\n"
     ]
    }
   ],
   "source": [
    "running_loss = []\n",
    "running_reward = []\n",
    "for iter_num in range(N_iter):\n",
    "    if iter_num % 10 == 0:\n",
    "        clear_output(wait=True)\n",
    "    samples_ind = np.unique(rng.integers(0, n_poses, size=N_episode))\n",
    "    samples_ind.sort()\n",
    "    samples = pl_to_enter[samples_ind]\n",
    "    place_ratios = np.zeros(len(samples))\n",
    "    \n",
    "    poses_true_info = run_true_simulation(init_lob, diffs_grouped, orders_per_diff, samples, place_ratios, rng)\n",
    "    poses_pred_info, obs_actions = run_pred_simulation(init_lob, diffs_grouped, orders_per_diff, samples, place_ratios, rl_agent)\n",
    "    #precalculate all costs and observation in needed format\n",
    "    all_episod_records = []\n",
    "    all_costs_td = []\n",
    "    all_costs_per_step = []\n",
    "    reward = 0\n",
    "    M = 0\n",
    "    for i, episod_record in enumerate(obs_actions):\n",
    "        all_episod_records.append(torch.FloatTensor(episod_record))\n",
    "        costs_per_step = torch.ones(len(episod_record)) * (2 * (poses_true_info[i] != poses_pred_info[i]) - 1) / len(episod_record)\n",
    "        if len(episod_record) > N_td:\n",
    "            all_costs_td.append((costs_per_step[:-1].unfold(0, N_td, 1) * gamma**torch.arange(0, N_td)).sum(dim=1))\n",
    "        else:\n",
    "            all_costs_td.append(torch.Tensor([]))\n",
    "        all_costs_per_step.append(costs_per_step)\n",
    "        if not ((poses_true_info[i] == poses_pred_info[i] and poses_true_info[i] == -1) or \\\n",
    "                (poses_true_info[i] == poses_pred_info[i] and poses_true_info[i] == None)):\n",
    "            if len(episod_record):\n",
    "                reward += poses_true_info[i] == poses_pred_info[i]\n",
    "                M += 1\n",
    "    reward /= M\n",
    "\n",
    "    all_episod_records = torch.concat(all_episod_records, dim=0)\n",
    "    # critic train\n",
    "    optimizer_critic = torch.optim.SGD(rl_critic.parameters(), lr=lr_critic)\n",
    "    for _ in tqdm(range(N_critic_epoch)):\n",
    "        #values = rl_critic(all_episod_records[:, :-1]) # last value - action\n",
    "        running_idx = 0\n",
    "        for i, episod_record in enumerate(obs_actions):\n",
    "            if not ((poses_true_info[i] == poses_pred_info[i] and poses_true_info[i] == -1) or \\\n",
    "                    (poses_true_info[i] == poses_pred_info[i] and poses_true_info[i] == None)):\n",
    "                if len(episod_record) > N_td:\n",
    "                    values = rl_critic(all_episod_records[running_idx:running_idx+len(episod_record), :-1]).reshape(-1, )\n",
    "                    optimizer_critic.zero_grad()\n",
    "                    loss = critic_objective_td(all_costs_td[i], \n",
    "                                            values,#values[running_idx:running_idx+len(episod_record)],\n",
    "                                            gamma, N_td)\n",
    "                    loss.backward()\n",
    "                    optimizer_critic.step()\n",
    "                    #print(loss, values)\n",
    "            running_idx += len(episod_record)\n",
    "    # agent train\n",
    "    optimizer_agent.zero_grad()\n",
    "    loss = actor_objective(all_episod_records, poses_true_info, poses_pred_info,\n",
    "                           obs_actions, all_costs_per_step, rl_agent, rl_critic, gamma)\n",
    "    loss.backward()\n",
    "    optimizer_agent.step()\n",
    "    # # PPO train\n",
    "    # loss = 0\n",
    "    # optimizer_agent = torch.optim.SGD(rl_agent.parameters(), lr=lr)\n",
    "    # for _ in tqdm(range(N_ppo_epoch)):\n",
    "    #     optimizer_agent.zero_grad()\n",
    "    #     loss_epoch = ppo_objective(all_episod_records, poses_true_info, poses_pred_info,\n",
    "    #                         obs_actions, all_costs_per_step, rl_agent, rl_critic, gamma, epsilon)\n",
    "    #     loss_epoch.backward()\n",
    "    #     optimizer_agent.step()\n",
    "    #     loss += loss_epoch.detach()\n",
    "    # loss /= N_ppo_epoch\n",
    "\n",
    "    running_loss.append(loss.item())\n",
    "    running_reward.append(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Critic MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:12<00:00,  2.44it/s]\n",
      "100%|██████████| 30/30 [00:11<00:00,  2.51it/s]\n",
      "100%|██████████| 30/30 [00:12<00:00,  2.44it/s]\n",
      "100%|██████████| 30/30 [00:11<00:00,  2.53it/s]\n",
      "100%|██████████| 30/30 [00:12<00:00,  2.49it/s]\n",
      "100%|██████████| 30/30 [00:12<00:00,  2.35it/s]\n",
      "100%|██████████| 30/30 [00:12<00:00,  2.45it/s]\n",
      "100%|██████████| 30/30 [00:13<00:00,  2.27it/s]\n",
      "100%|██████████| 30/30 [00:12<00:00,  2.45it/s]\n",
      "100%|██████████| 30/30 [00:11<00:00,  2.53it/s]\n"
     ]
    }
   ],
   "source": [
    "running_loss = []\n",
    "running_reward = []\n",
    "for iter_num in range(N_iter):\n",
    "    if iter_num % 10 == 0:\n",
    "        clear_output(wait=True)\n",
    "    samples_ind = np.unique(rng.integers(0, n_poses, size=N_episode))\n",
    "    samples_ind.sort()\n",
    "    samples = pl_to_enter[samples_ind]\n",
    "    place_ratios = np.zeros(len(samples))\n",
    "    \n",
    "    poses_true_info = run_true_simulation(init_lob, diffs_grouped, orders_per_diff, samples, place_ratios, rng)\n",
    "    poses_pred_info, obs_actions = run_pred_simulation(init_lob, diffs_grouped, orders_per_diff, samples, place_ratios, rl_agent)\n",
    "    #precalculate all costs and observation in needed format\n",
    "    all_episod_records = []\n",
    "    all_costs_mse = []\n",
    "    all_costs_per_step = []\n",
    "    reward = 0\n",
    "    M = 0\n",
    "    for i, episod_record in enumerate(obs_actions):\n",
    "        all_episod_records.append(torch.FloatTensor(episod_record))\n",
    "        costs_per_step = torch.ones(len(episod_record)) * (2 * (poses_true_info[i] != poses_pred_info[i]) - 1) / len(episod_record)\n",
    "        if len(episod_record):\n",
    "            episod_cost = (2 * (poses_true_info[i] != poses_pred_info[i]) - 1) / len(episod_record)\n",
    "            coefs = gamma**torch.arange(0, len(episod_record))\n",
    "            cum_sum = torch.cumsum(coefs, dim=0)\n",
    "            reverse_cum_sum = coefs - cum_sum + cum_sum[-1]\n",
    "            all_costs_mse.append(reverse_cum_sum / gamma**torch.arange(0, len(episod_record)) * episod_cost)\n",
    "        else:\n",
    "            all_costs_mse.append(torch.FloatTensor([]))\n",
    "        all_costs_per_step.append(costs_per_step)\n",
    "        if not ((poses_true_info[i] == poses_pred_info[i] and poses_true_info[i] == -1) or \\\n",
    "                (poses_true_info[i] == poses_pred_info[i] and poses_true_info[i] == None)):\n",
    "            if len(episod_record):\n",
    "                reward += poses_true_info[i] == poses_pred_info[i]\n",
    "                M += 1\n",
    "    reward /= M\n",
    "\n",
    "    all_episod_records = torch.concat(all_episod_records, dim=0)\n",
    "    # critic train\n",
    "\n",
    "    for _ in tqdm(range(N_critic_epoch)):\n",
    "        #values = rl_critic(all_episod_records[:, :-1]) # last value - action\n",
    "        running_idx = 0\n",
    "\n",
    "        for i, episod_record in enumerate(obs_actions):\n",
    "            if not ((poses_true_info[i] == poses_pred_info[i] and poses_true_info[i] == -1) or \\\n",
    "                    (poses_true_info[i] == poses_pred_info[i] and poses_true_info[i] == None)):\n",
    "                if len(episod_record):\n",
    "                    values = rl_critic(all_episod_records[running_idx:running_idx+len(episod_record), :-1]).reshape(-1, )\n",
    "                    optimizer_critic.zero_grad()\n",
    "                    loss = critic_objective_mse(all_costs_mse[i], \n",
    "                                            values)\n",
    "                    loss.backward()\n",
    "                    optimizer_critic.step()\n",
    "\n",
    "                    #print(loss, values)\n",
    "\n",
    "            running_idx += len(episod_record)\n",
    "    # agent train\n",
    "    optimizer_agent.zero_grad()\n",
    "    loss = actor_objective(all_episod_records, poses_true_info, poses_pred_info,\n",
    "                           obs_actions, all_costs_per_step, rl_agent, rl_critic, gamma)\n",
    "    loss.backward()\n",
    "    optimizer_agent.step()\n",
    "    # # PPO train\n",
    "    # optimizer_agent.zero_grad()\n",
    "    #loss = ppo_objective(all_episod_records, poses_true_info, poses_pred_info,\n",
    "    #                       obs_actions, all_costs_per_step, rl_agent, rl_critic, gamma, epsilon)\n",
    "    # loss.backward()\n",
    "    # optimizer_agent.step()\n",
    "\n",
    "    running_loss.append(loss.item())\n",
    "    running_reward.append(reward)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
